{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio 1 - Extracting linguistic features using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By Sofie Mosegaard, 22-02-2024*\n",
    "\n",
    "This assignment concerns using spaCy to extract linguistic information from a corpus of texts.\n",
    "This assignment is designed to test that you can:\n",
    "\n",
    "1. Work with multiple input data arranged hierarchically in folders;\n",
    "2. Use spaCy to extract linguistic information from text data;\n",
    "3. Save those results in a clear way which can be shared or used for future analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work/Language_analytics/cds-lang/nbs/assignment_drafts/assignment1_portfolio1.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://app-5042996-0.cloud.sdu.dk/work/Language_analytics/cds-lang/nbs/assignment_drafts/assignment1_portfolio1.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m \n\u001b[1;32m      <a href='vscode-notebook-cell://app-5042996-0.cloud.sdu.dk/work/Language_analytics/cds-lang/nbs/assignment_drafts/assignment1_portfolio1.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# python -m spacy download en_core_web_md\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5042996-0.cloud.sdu.dk/work/Language_analytics/cds-lang/nbs/assignment_drafts/assignment1_portfolio1.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_md\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[1;32m    464\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 501\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, enable\u001b[39m=\u001b[39;49menable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/en_core_web_md/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moverrides)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    683\u001b[0m     data_path,\n\u001b[1;32m    684\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    685\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    686\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    687\u001b[0m     enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m    688\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    689\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    690\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:547\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    538\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[1;32m    539\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[39m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     meta\u001b[39m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[0;32m--> 547\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39;49mfrom_disk(model_path, exclude\u001b[39m=\u001b[39;49mexclude, overrides\u001b[39m=\u001b[39;49moverrides)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py:2209\u001b[0m, in \u001b[0;36mLanguage.from_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (path \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mexists() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m# Convert to list here in case exclude is (default) tuple\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     exclude \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(exclude) \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m-> 2209\u001b[0m util\u001b[39m.\u001b[39;49mfrom_disk(path, deserializers, exclude)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   2210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m path  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_link_components()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py:1390\u001b[0m, in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[39mfor\u001b[39;00m key, reader \u001b[39min\u001b[39;00m readers\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1388\u001b[0m     \u001b[39m# Split to support file names like meta.json\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m exclude:\n\u001b[0;32m-> 1390\u001b[0m         reader(path \u001b[39m/\u001b[39;49m key)\n\u001b[1;32m   1391\u001b[0m \u001b[39mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py:2195\u001b[0m, in \u001b[0;36mLanguage.from_disk.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m   2193\u001b[0m deserializers[\u001b[39m\"\u001b[39m\u001b[39mmeta.json\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m deserialize_meta  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m deserializers[\u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m deserialize_vocab  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 2195\u001b[0m deserializers[\u001b[39m\"\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m p: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mfrom_disk(  \u001b[39m# type: ignore[union-attr]\u001b[39;49;00m\n\u001b[1;32m   2196\u001b[0m     p, exclude\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mvocab\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2197\u001b[0m )\n\u001b[1;32m   2198\u001b[0m \u001b[39mfor\u001b[39;00m name, proc \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_components:\n\u001b[1;32m   2199\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m exclude:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx:778\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.from_disk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx:846\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.from_bytes\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx:123\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.rules.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx:571\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer._load_special_cases\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx:606\u001b[0m, in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/vocab.pyx:280\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.make_fused_token\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/vocab.pyx:179\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab.get_by_orth\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/vocab.pyx:202\u001b[0m, in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/lang/lex_attrs.py:144\u001b[0m, in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    140\u001b[0m             shape\u001b[39m.\u001b[39mappend(shape_char)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(shape)\n\u001b[0;32m--> 144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlower\u001b[39m(string: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m string\u001b[39m.\u001b[39mlower()\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprefix\u001b[39m(string: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "# python -m spacy download en_core_web_md\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the relative frequency per 10,000 words and round the decimals\n",
    "# The input is the number of POS and the total number of tokens in the given text, while the \n",
    "# output will be the he relative frquency per 10,000 words\n",
    "\n",
    "def rel_freq(count, len_doc): \n",
    "    return round((count/len_doc * 10000), 2)\n",
    "\n",
    "# Count total number of unique PER, LOC, and ORG entities\n",
    "# The input is a spacy doc object, while the output will be the total number of unique persons, locations (LOC),\n",
    "# and organisations (ORG) mentioned in the specified, input doc object.\n",
    "\n",
    "def no_unique_ents(doc):\n",
    "    enteties = []\n",
    "\n",
    "    for ent in doc.ents: \n",
    "        enteties.append((ent.text, ent.label_))\n",
    "\n",
    "    enteties_df = pd.DataFrame(enteties, columns=[\"enteties\", \"label\"])\n",
    "    enteties_df = enteties_df.drop_duplicates()\n",
    "    unique_counts = enteties_df.value_counts(subset = \"label\")\n",
    "    \n",
    "    unique_labels = ['PERSON', 'LOC', 'ORG']\n",
    "    unique_row = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        if label in (unique_counts.index):\n",
    "            unique_row.append(unique_counts[label])\n",
    "        else:\n",
    "            unique_row.append(0)\n",
    "\n",
    "    return unique_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting linguistic features using spaCy \n",
    "For each text in subfolders of the folder 'in', I will extract linguistic features and append it to a subfolder-specific table. In the end, a .csv file for each subfolder will be created and saved in the folder 'out'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, specify the filepath to the folder with all the data\n",
    "filepath = os.path.join(\n",
    "                        \"..\",\n",
    "                        \"in\",\n",
    "                        \"USEcorpus\"\n",
    "                        )\n",
    "\n",
    "# Loop over each of the 14 subfolders (a1, a2, a3...)\n",
    "for subfolder in sorted(os.listdir(filepath)): # sorted = loops through the subfolders in the original, sorted order\n",
    "    subfolder_path = os.path.join(filepath, subfolder)\n",
    "\n",
    "    if os.path.isdir(subfolder_path): # Check if the specified directory exists or nor\n",
    "\n",
    "        # Create a pandas dataframe for each subfolder with specified column names\n",
    "        out_df = pd.DataFrame(columns=(\"Filename\",\n",
    "                                        \"RelFreq NOUN\",\n",
    "                                        \"RelFreq VERB\",\n",
    "                                        \"RelFreq ADJ\",\n",
    "                                        \"RelFreq ADV\",\n",
    "                                        \"No. Unique PER\",\n",
    "                                        \"No. Unique LOC\",\n",
    "                                        \"No. Unique ORG\"))\n",
    "\n",
    "        # Loop over each text file in the subfolder\n",
    "        for file in glob.glob(os.path.join(subfolder_path, \"*.txt\")):\n",
    "            if os.path.isfile(file): # Function checks whether there excists files on the specified path\n",
    "                \n",
    "                with open(file, \"r\", encoding = \"latin-1\") as f:\n",
    "                    text = f.read()\n",
    "\n",
    "                    text = re.sub(r\"<*?>\", \"\", text) # Remove metadata between <> and replace it with \"\" (= nothing hehe)\n",
    "\n",
    "                    doc = nlp(text) # Create spacy doc\n",
    "\n",
    "                    # Count number of each POS\n",
    "                    nouns_count, verbs_count, adverb_count, adjective_count = 0, 0, 0, 0\n",
    "\n",
    "                    for token in doc:\n",
    "                        if token.pos_ == \"NOUN\":\n",
    "                            nouns_count += 1\n",
    "                        elif token.pos_ == \"VERB\":\n",
    "                            verbs_count += 1\n",
    "                        elif token.pos_ == \"ADV\":\n",
    "                            adverb_count += 1\n",
    "                        elif token.pos_ == \"ADJ\":\n",
    "                            adjective_count += 1\n",
    "            \n",
    "                    nouns_relative_freq = rel_freq(nouns_count, len(doc))\n",
    "                    verbs_relative_freq = rel_freq(verbs_count, len(doc))\n",
    "                    adjective_relative_freq = rel_freq(adjective_count, len(doc))\n",
    "                    adverb_relative_freq = rel_freq(adverb_count, len(doc))\n",
    "                    \n",
    "                    # Count total number of unique PER, LOC, and ORG entities\n",
    "                    No_unique_per, No_unique_loc, No_unique_org = no_unique_ents(doc)\n",
    "\n",
    "                    # Append the name of the text to the filenames folder \n",
    "                    text_name = file.split(\"/\")[-1]\n",
    "\n",
    "                    # Append the extracted linguistic features for each text to a row in the out_df\n",
    "                    text_row = [text_name, nouns_relative_freq,\n",
    "                                verbs_relative_freq, adjective_relative_freq,\n",
    "                                adverb_relative_freq, No_unique_per,\n",
    "                                No_unique_loc, No_unique_org]\n",
    "\n",
    "                    out_df.loc[len(out_df)] = text_row\n",
    "            \n",
    "            # Specify path to the output folcder and name of the specific .csv file\n",
    "            csv_outpath = os.path.join(\"..\", \"out\", f\"{subfolder}_data.csv\")\n",
    "\n",
    "        out_df.to_csv(csv_outpath)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - import one .csv file to check how the table looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Filename</th>\n",
       "      <th>RelFreq NOUN</th>\n",
       "      <th>RelFreq VERB</th>\n",
       "      <th>RelFreq ADJ</th>\n",
       "      <th>RelFreq ADV</th>\n",
       "      <th>No. Unique PER</th>\n",
       "      <th>No. Unique LOC</th>\n",
       "      <th>No. Unique ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1112.a1.txt</td>\n",
       "      <td>1273.96</td>\n",
       "      <td>1465.61</td>\n",
       "      <td>631.34</td>\n",
       "      <td>823.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0107.a1.txt</td>\n",
       "      <td>1204.99</td>\n",
       "      <td>1440.44</td>\n",
       "      <td>886.43</td>\n",
       "      <td>609.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1071.a1.txt</td>\n",
       "      <td>1396.71</td>\n",
       "      <td>1302.82</td>\n",
       "      <td>622.07</td>\n",
       "      <td>481.22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0191.a1.txt</td>\n",
       "      <td>1251.49</td>\n",
       "      <td>1358.76</td>\n",
       "      <td>750.89</td>\n",
       "      <td>679.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3064.a1.txt</td>\n",
       "      <td>1538.46</td>\n",
       "      <td>1372.14</td>\n",
       "      <td>665.28</td>\n",
       "      <td>665.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>0149.a1.txt</td>\n",
       "      <td>1239.11</td>\n",
       "      <td>1239.11</td>\n",
       "      <td>735.72</td>\n",
       "      <td>735.72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>299</td>\n",
       "      <td>0100.a1.txt</td>\n",
       "      <td>1524.48</td>\n",
       "      <td>1216.78</td>\n",
       "      <td>797.20</td>\n",
       "      <td>531.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>300</td>\n",
       "      <td>3045.a1.txt</td>\n",
       "      <td>1144.58</td>\n",
       "      <td>1385.54</td>\n",
       "      <td>622.49</td>\n",
       "      <td>401.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>301</td>\n",
       "      <td>0128.a1.txt</td>\n",
       "      <td>1355.93</td>\n",
       "      <td>1210.65</td>\n",
       "      <td>641.65</td>\n",
       "      <td>726.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>302</td>\n",
       "      <td>1062.a1.txt</td>\n",
       "      <td>1302.38</td>\n",
       "      <td>1200.45</td>\n",
       "      <td>566.25</td>\n",
       "      <td>554.93</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     Filename  RelFreq NOUN  RelFreq VERB  RelFreq ADJ  \\\n",
       "0             0  1112.a1.txt       1273.96       1465.61       631.34   \n",
       "1             1  0107.a1.txt       1204.99       1440.44       886.43   \n",
       "2             2  1071.a1.txt       1396.71       1302.82       622.07   \n",
       "3             3  0191.a1.txt       1251.49       1358.76       750.89   \n",
       "4             4  3064.a1.txt       1538.46       1372.14       665.28   \n",
       "..          ...          ...           ...           ...          ...   \n",
       "298         298  0149.a1.txt       1239.11       1239.11       735.72   \n",
       "299         299  0100.a1.txt       1524.48       1216.78       797.20   \n",
       "300         300  3045.a1.txt       1144.58       1385.54       622.49   \n",
       "301         301  0128.a1.txt       1355.93       1210.65       641.65   \n",
       "302         302  1062.a1.txt       1302.38       1200.45       566.25   \n",
       "\n",
       "     RelFreq ADV  No. Unique PER  No. Unique LOC  No. Unique ORG  \n",
       "0         823.00               0               0               0  \n",
       "1         609.42               0               0               0  \n",
       "2         481.22               3               0               4  \n",
       "3         679.38               0               0               2  \n",
       "4         665.28               0               0               0  \n",
       "..           ...             ...             ...             ...  \n",
       "298       735.72               1               0               0  \n",
       "299       531.47               0               0               0  \n",
       "300       401.61               0               1               0  \n",
       "301       726.39               0               0               0  \n",
       "302       554.93               8               0               2  \n",
       "\n",
       "[303 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_tester = pd.read_csv(\"../out/a1_data.csv\")\n",
    "table_tester"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
