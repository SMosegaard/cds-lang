{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio 2 - script 1 - tfidf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By Sofie Mosegaard, 07-03-2024*\n",
    "\n",
    "In this script, the data will be vectorized and the new feature extracted data will be saved as objects. By doing so, I will only vectorize the data once in total instead of once per script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System tools\n",
    "import os\n",
    "import sys\n",
    "import scipy as sp\n",
    "\n",
    "# Data munging tools\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning packages\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# For saving\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to pandas csv\n",
    "filepath = os.path.join(\n",
    "                        \"..\",\n",
    "                        \"in\",\n",
    "                        \"fake_or_real_news.csv\"\n",
    "                        )\n",
    "\n",
    "data = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data variables\n",
    "X = data[\"text\"]\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a 80:20 train:test split in the data using the input X (the text for the model) and y (the classification labels). To ensure reproducibility, a random state of 123 is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,    \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to create a vectorizer object. The vectorizer object will be on unigrams and bigrams, make everything lowercase to ensure alignment, remove the very common words (top 5%) and very rare words (bottom 5%), as well as keep the top 500 features.\n",
    "\n",
    "The vectorizer object will then be used to transform the text data into vectors of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer object\n",
    "vectorizer = TfidfVectorizer(ngram_range = (1,2),\n",
    "                             lowercase =  True,\n",
    "                             max_df = 0.95,\n",
    "                             min_df = 0.05,\n",
    "                             max_features = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the vectorizer object to the training data\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Fit the vectorizer object to the test data\n",
    "X_test_features = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the vectorizer and feature extracted objects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As vectorizing and feature extracting can be very time consuming on larger datasets, I will save the vectorizer and the vectorized feature extracted objects. By doing so, we can simply load in the objects and use the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the vectorizer\n",
    "dump(vectorizer, \"../models/tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the spicy sparse matrixs objects (X_train_features and X_test_features)\n",
    "sp.sparse.save_npz('../models/X_train_features_sparse_matrix.npz', X_train_features)\n",
    "sp.sparse.save_npz('../models/X_test_features_sparse_matrix.npz', X_test_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
