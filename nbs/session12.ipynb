{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12 - Measuring environmental impact\n",
    "\n",
    "In this session, we're going to look at one particular way that we can measure the impact of our code on the world around us. In particular, we're going to be looking at how we can approximate the *environmental impact* of our cultural data science footprint.\n",
    "\n",
    "To do this, we're going to use the open-source software package *CodeCarbon*. You can find more information at the following links:\n",
    "\n",
    "- CodeCarbon Website: [https://codecarbon.io/](https://codecarbon.io/)\n",
    "- GitHub Repo: [https://mlco2.github.io/codecarbon/](https://mlco2.github.io/codecarbon/)\n",
    "- Documentation: [https://mlco2.github.io/codecarbon/](https://mlco2.github.io/codecarbon/)\n",
    "\n",
    "We'll do some testing on HuggingFace pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing HuggingFace pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:35:01.995116: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 10:35:01.999190: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 10:35:02.052440: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 10:35:02.945307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n"
     ]
    }
   ],
   "source": [
    "# pip install codecarbon transformers datasets pandas tqdm tensorflow tf-keras\n",
    "\n",
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text summarization pipeline__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember from a couple of weeks ago that *text summarization* was quite a compute intensive task. So let's see exactly how compute intensive it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "In the former task our best model outperforms even all previously reported ensembles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7c899f54854c41aaab81d9f06d404d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7c2f42d1ba4b8b9745555829a54d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cbe3ddb7574d4ea50f43b67dbac7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674b816e65834ba2b4f84c74a1fbf6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7d068eca934a119045d59d192d8b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline(task = \"summarization\", \n",
    "                      min_length = 10,\n",
    "                      max_length = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different ways that we can work with CodeCarbon, all of which is clearly explained in the relevant documentation.\n",
    "\n",
    "We'll go through each of them one at a time here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Creating a tracker object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:17] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:35:17] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:35:17] No GPU found.\n",
      "[codecarbon INFO @ 10:35:17] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:35:17] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:35:18] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:35:18] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:18] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:35:18]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:35:18]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:35:18]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:35:18]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:35:18]   CPU count: 64\n",
      "[codecarbon INFO @ 10:35:18]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:18]   GPU count: None\n",
      "[codecarbon INFO @ 10:35:18]   GPU model: None\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714034122.456025    2516 service.cc:145] XLA service 0x5586282404a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1714034122.456089    2516 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "2024-04-25 10:35:22.462357: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1714034122.508992    2516 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[codecarbon INFO @ 10:35:29] Energy consumed for RAM : 0.000317 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:35:29] Energy consumed for all CPUs : 0.000095 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:35:29] 0.000412 kWh of electricity used since the beginning.\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.4379197294264e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker = EmissionsTracker() # initialise \n",
    "tracker.start()\n",
    "summary = summarizer(text)\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:34] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:35:34] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:35:34] No GPU found.\n",
      "[codecarbon INFO @ 10:35:34] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:35:34] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:35:35] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:35:35] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:35] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:35:35]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:35:35]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:35:35]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:35:35]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:35:35]   CPU count: 64\n",
      "[codecarbon INFO @ 10:35:35]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:35]   GPU count: None\n",
      "[codecarbon INFO @ 10:35:35]   GPU model: None\n",
      "[codecarbon INFO @ 10:35:45] Energy consumed for RAM : 0.000287 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:35:45] Energy consumed for all CPUs : 0.000086 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:35:45] 0.000373 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'the Transformer replaces recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention .'}]\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    summary = summarizer(text)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - A Python decoractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import track_emissions\n",
    "\n",
    "@track_emissions\n",
    "def summarization(text):\n",
    "    summary = summarizer(text)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:40:28] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:40:28] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:40:28] No GPU found.\n",
      "[codecarbon INFO @ 10:40:28] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:40:28] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:40:29] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:40:29] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:40:29] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:40:29]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:40:29]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:40:29]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:40:29]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:40:29]   CPU count: 64\n",
      "[codecarbon INFO @ 10:40:29]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:40:29]   GPU count: None\n",
      "[codecarbon INFO @ 10:40:29]   GPU model: None\n",
      "[codecarbon INFO @ 10:40:39] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 10:40:39] Energy consumed for RAM : 0.000258 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:40:39] Energy consumed for all CPUs : 0.000078 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:40:39] 0.000336 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:40:39] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'the Transformer replaces recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention .'}]\n"
     ]
    }
   ],
   "source": [
    "summarization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example\n",
    "\n",
    "We can make the results more useful by changing the tracker parameters - full list can be found here [https://mlco2.github.io/codecarbon/parameters.html](https://mlco2.github.io/codecarbon/parameters.html).\n",
    "\n",
    "In the example that follows, we're going to download a HuggingFace dataset and a pretrained emotion classification model. \n",
    "\n",
    "We also introduce specific *tasks* to more clearly understand the impact of different parts of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:51:04] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:51:04] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:51:04] No GPU found.\n",
      "[codecarbon INFO @ 10:51:04] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:51:04] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:51:05] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:51:05] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:51:05] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:51:05]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:51:05]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:51:05]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:51:05]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:51:05]   CPU count: 64\n",
      "[codecarbon INFO @ 10:51:05]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:51:05]   GPU count: None\n",
      "[codecarbon INFO @ 10:51:05]   GPU model: None\n",
      "[codecarbon INFO @ 10:51:15] Energy consumed for RAM : 0.000257 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:51:15] Energy consumed for all CPUs : 0.000077 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:51:15] 0.000334 kWh of electricity used since the beginning.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "[codecarbon INFO @ 10:51:20] Energy consumed for RAM : 0.000457 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:51:20] Energy consumed for all CPUs : 0.000137 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:51:20] 0.000594 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e83b00548a4d00bc236697c1c4e6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:51:44] Energy consumed for RAM : 0.001421 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:51:44] Energy consumed for all CPUs : 0.000428 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:51:44] 0.001849 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:51:44] Energy consumed for RAM : 0.001421 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:51:44] Energy consumed for all CPUs : 0.000428 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:51:44] 0.001849 kWh of electricity used since the beginning.\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:197: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0003335988430101867"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfolder = os.path.join(\"emissions\")\n",
    "#os.mkdir(outfolder) # if the folder already excists, it will return an error\n",
    " \n",
    "tracker = EmissionsTracker(project_name = \"sentiment classification\",\n",
    "                           experiment_id = \"sentiment_classifier\",\n",
    "                           output_dir = outfolder,\n",
    "                           output_file = \"emissions_sentiment.csv\")\n",
    "\n",
    "# tracking data downloading\n",
    "tracker.start_task(\"load dataset\")\n",
    "dataset = datasets.load_dataset(\"imdb\", split = \"test\")\n",
    "imdb_emissions = tracker.stop_task()\n",
    "\n",
    "# tracking downloading and initializing model\n",
    "tracker.start_task(\"build model\")\n",
    "classifier = pipeline(task = \"sentiment-analysis\", model = \"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "model_emissions = tracker.stop_task()\n",
    "\n",
    "# tracking classification pipeline\n",
    "tracker.start_task(\"run classification\")\n",
    "preds = []\n",
    "for row in tqdm(dataset[\"text\"][:1000]):\n",
    "    preds.append(classifier(row[:100]))\n",
    "classifier_emissions = tracker.stop_task()\n",
    "\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspecting the results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- Now that you have the basics down, head over and consider Assignment 5!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
